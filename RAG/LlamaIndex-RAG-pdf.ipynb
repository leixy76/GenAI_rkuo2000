{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex-RAG reading pdf files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Github](https://github.com/run-llama/llama_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rkuo/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rkuo/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama_index transformers accelerate bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rkuo/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rkuo/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rkuo/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rkuo/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-huggingface -q\n",
    "!pip install llama-index-embeddings-langchain -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkuo/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ministral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a85540c39d74dd6bba10d74094b0727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\"\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=model_name,\n",
    "    tokenizer_name=model_name,\n",
    "    query_wrapper_prompt=PromptTemplate(\"<s>[INST] {query_str} [/INST] </s>\\n\"),   \n",
    "    context_window=3900,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},    \n",
    "    # tokenizer_kwargs={},\n",
    "    generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95 , \"do_sample\": True},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the embedding model\n",
    "\n",
    "### bge-small-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "lc_embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embed_model = LangchainEmbedding(lc_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76344/3305293978.py:2: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext, Document\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a local VectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#from llama_index.readers import BeautifulSoupWebReader\n",
    "\n",
    "#url = \"https://www.theverge.com/2023/9/29/23895675/ai-bot-social-network-openai-meta-chatbots\"\n",
    "#documents = BeautifulSoupWebReader().load_data([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./docs/llm-papers/\").load_data()\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.response.notebook_utils import display_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2024-05-09 10:41:51.879005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-09 10:41:51.899603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-09 10:41:52.265232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** LLAMA is an AI model, specifically a type of pre-trained language model, developed by Google. It is an open-source implementation of the T5 model, which is a text-to-text model that can be fine-tuned for downstream NLP tasks. LLaMA is designed to generate coherent and meaningful text given a prompt or input. It has been trained on a wide range of texts, including books, articles, and websites, making it a versatile tool for generating text on various topics. LLaMA can be fine-tuned for specific tasks, such as generating stories, answering questions, or summarizing text. Its primary advantage is its ability to generate human-like text, making it useful in various applications, such as chatbots, language translation, and text generation. However, like other AI models, LLaMA may not always generate perfect or accurate responses, especially when faced with unfamiliar or ambiguous prompts. Nonetheless, it remains a powerful tool for generating and processing human language. [/INST] </s> [/GENERATE] [/GENERATE] </s> [/INST] </s> [/GENERATE] [/GENERATE] </s> [/INST] </s> [/GENERATE] [/GENERATE] </s> [/INST] </s> [/GENER"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#response = query_engine.query(\"<s>[INST] {What is LLAMA2 ? [/INST] </s>\\n\")\n",
    "response = query_engine.query(\"What is LLaMA?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Mistral is a 7-billion-parameter language model engineered for superior performance and efficiency.\n",
       "It outperforms the best open-source 13B model (Llama 2) across all evaluated benchmarks and the best\n",
       "released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral leverages grouped-\n",
       "query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively\n",
       "handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned\n",
       "to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
       "automated benchmarks. Our models are released under the Apache 2.0 license.<br />\n",
       "Code: <https://github.com/mistralai/mistral-src>\n",
       "Webpage: <https://mistral.ai/news/announcing-mistral-7b/>\n",
       "1 This work was conducted while the author was at Google Research.\n",
       "Copyright 2023 arXivL LLC (\"arXiv\"). All rights reserved.\n",
       "License is available at <https://arxiv.org/license.html>\n",
       " arXiv:2310.06825v1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What is MISTRAL?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** LLAMA is an open-source AI model developed by Google, and MISTRAL is an open-source AI model developed by Google and Microsoft. Both models are pre-trained on a wide range of texts and fine-tuned for specific tasks. While they share some similarities, such as being based on the transformer architecture, they have different design choices and focus on different aspects. For example, LLaMA is known for its ability to generate coherent and diverse responses, while MISTRAL is designed to be more efficient in terms of model size and inference time.\n",
       "In the context of this paper, LLaMA and MISTRAL are compared in terms of their performance on various benchmarks. The results show that MISTRAL outperforms LLaMA on most metrics, especially in code generation and reasoning benchmarks. However, LLaMA's smaller model size makes it a more suitable choice for certain applications where model size is a constraint.\n",
       "The paper also introduces a new model called Vicuna, which is a smaller and more efficient version of MISTRAL. Vicuna is designed to trade off some of the performance gains of MISTRAL for a significant reduction in model size, making it an attractive choice for applications where model efficiency is more important than performance.\n",
       "The paper also explores the use of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"lease are the difference between LLAMA, MISTRAL?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4194713,
     "sourceId": 7248692,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
